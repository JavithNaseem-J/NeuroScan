{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c49dbb13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "940cb37d",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6da9d151",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from typing import List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "65ed29e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ModelEvaluationConfig:\n",
    "    root_dir: Path\n",
    "    test_gen_path: Path\n",
    "    metrics_file_path: str\n",
    "    confusion_matrix_plot_path: str\n",
    "    mlflow_uri: str\n",
    "    experiment_name: str\n",
    "    batch_size: int\n",
    "    model_file: Path\n",
    "    target_image_size: list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a29a82e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from NeuroScan.utils.helpers import *\n",
    "from NeuroScan.constants.paths import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "83a83e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelConfigurationManager:\n",
    "    \"\"\"Manages configuration loading for model evaluation.\"\"\"\n",
    "    def __init__(self, config_file=CONFIG_PATH, params_file=PARAMS_PATH):\n",
    "        self.config = read_yaml(config_file)\n",
    "        self.params = read_yaml(params_file)\n",
    "        create_directories([self.config.artifacts_root])\n",
    "\n",
    "    def get_model_evaluation_config(self) -> ModelEvaluationConfig:\n",
    "        config = self.config.evaluation\n",
    "        params = self.params.transform\n",
    "        create_directories([config.root_dir])\n",
    "        return ModelEvaluationConfig(\n",
    "            root_dir=Path(config.root_dir),\n",
    "            test_gen_path=Path(config.test_gen_path),\n",
    "            metrics_file_path=config.metrics_file_path,\n",
    "            model_file=Path(config.model_file),\n",
    "            confusion_matrix_plot_path=config.confusion_matrix_plot_path,\n",
    "            mlflow_uri=config.mlflow_uri,\n",
    "            experiment_name=config.experiment_name,\n",
    "            batch_size=params.batch_size,\n",
    "            target_image_size=params.target_image_size\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47a847d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "import mlflow\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, classification_report, f1_score, recall_score, precision_score,ConfusionMatrixDisplay\n",
    "\n",
    "class ModelEvaluator:\n",
    "\n",
    "\n",
    "    def __init__(self, config: ModelEvaluationConfig):\n",
    "        self.config = config\n",
    "        self.model = tf.keras.models.load_model(self.config.model_file)\n",
    "        self.test_generator = None\n",
    "        mlflow.set_tracking_uri(self.config.mlflow_uri)\n",
    "        mlflow.set_experiment(self.config.experiment_name)\n",
    "        self.run = mlflow.start_run()\n",
    "        self._initialize_test_generator()\n",
    "\n",
    "\n",
    "    def _initialize_test_generator(self):\n",
    "\n",
    "        try:\n",
    "            test_data = np.load(self.config.test_gen_path, allow_pickle=True).item()\n",
    "            if test_data['data'].shape[1:3] != tuple(self.config.target_image_size):\n",
    "                logger.warning(f\"Resizing test data from {test_data['data'].shape[1:3]} to {self.config.target_image_size}\")\n",
    "                test_data['data'] = np.array([tf.image.resize(img, self.config.target_image_size).numpy() for img in test_data['data']])\n",
    "            self.test_generator = tf.data.Dataset.from_tensor_slices((test_data['data'], test_data['labels'])).batch(self.config.batch_size)\n",
    "\n",
    "            logger.info(f\"Test generator initialized with {self.test_generator.cardinality().numpy() * self.config.batch_size} samples.\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error initializing test generator: {e}\")\n",
    "            raise\n",
    "\n",
    "    def evaluate(self):\n",
    "        \"\"\"Evaluates the model on the test data, saves metrics and plot.\"\"\"\n",
    "        try:\n",
    "            logger.info(\"Evaluating model on test data...\")\n",
    "            test_steps = self.test_generator.cardinality().numpy()\n",
    "            y_true = []\n",
    "            y_pred = []\n",
    "            for x_batch, y_batch in self.test_generator.take(test_steps):\n",
    "                y_pred_batch = self.model.predict(x_batch)\n",
    "                y_true.extend(np.argmax(y_batch, axis=1))\n",
    "                y_pred.extend(np.argmax(y_pred_batch, axis=1))\n",
    "\n",
    "            cm = confusion_matrix(y_true, y_pred)\n",
    "            report = classification_report(y_true, y_pred, output_dict=True)\n",
    "            f1 = f1_score(y_true, y_pred, average='weighted')\n",
    "            recall = recall_score(y_true, y_pred, average='weighted')\n",
    "            precision = precision_score(y_true, y_pred, average='weighted')\n",
    "            test_loss, test_accuracy = self.model.evaluate(self.test_generator, verbose=1)\n",
    "\n",
    "            metrics = {\n",
    "                \"test_loss\": float(test_loss),\n",
    "                \"test_accuracy\": float(test_accuracy),\n",
    "                \"f1_score\": float(f1),\n",
    "                \"recall\": float(recall),\n",
    "                \"precision\": float(precision)\n",
    "            }\n",
    "\n",
    "            with open(self.config.metrics_file_path, 'w') as f:\n",
    "                json.dump(metrics, f, indent=4)\n",
    "            logger.info(f\"Metrics saved to {self.config.metrics_file_path}\")\n",
    "\n",
    "            confusion_matrix_plot_path = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['glioma', 'meningioma', 'notumor', 'pituitary'])\n",
    "            confusion_matrix_plot_path.plot(cmap='Blues', values_format='d')\n",
    "            plt.title('Confusion Matrix')\n",
    "            plt.savefig(self.config.confusion_matrix_plot_path)\n",
    "            plt.close()\n",
    "            logger.info(f\"Confusion matrix plot saved to {self.config.confusion_matrix_plot_path}\")\n",
    "\n",
    "            # Log metrics and artifacts to MLflow\n",
    "            mlflow.log_metric(\"test_loss\", test_loss)\n",
    "            mlflow.log_metric(\"test_accuracy\", test_accuracy)\n",
    "            mlflow.log_metric(\"f1_score\", f1)\n",
    "            mlflow.log_metric(\"recall\", recall)\n",
    "            mlflow.log_metric(\"precision\", precision)\n",
    "            mlflow.log_artifact(self.config.metrics_file_path)\n",
    "            mlflow.log_artifact(self.config.confusion_matrix_plot_path)\n",
    "            mlflow.log_text(str(cm), \"confusion_matrix.txt\")\n",
    "\n",
    "            logger.info(f\"Test Loss: {test_loss}, Test Accuracy: {test_accuracy}, \"\n",
    "                        f\"F1-Score: {f1:.4f}, Recall: {recall:.4f}, Precision: {precision:.4f}\")\n",
    "            logger.info(f\"Confusion Matrix:\\n{cm}\")\n",
    "            return test_loss, test_accuracy, cm, report\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error during evaluation: {e}\")\n",
    "            raise\n",
    "        finally:\n",
    "            mlflow.end_run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62ed2c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "        model_config = ModelConfigurationManager()\n",
    "        eval_config = model_config.get_model_evaluation_config()\n",
    "        evaluator = ModelEvaluator(config=eval_config)\n",
    "        test_loss, test_accuracy, cm, report = evaluator.evaluate()\n",
    "except Exception as e:\n",
    "        logger.error(f\"Model evaluation failed: {e}\")\n",
    "        raise"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NeuroScan",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
