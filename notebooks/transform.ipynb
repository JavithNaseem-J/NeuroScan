{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b5bf3ec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e955dcf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f10be191",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(os.path.join(os.getcwd(), 'src'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f0e76dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from typing import Tuple\n",
    "from NeuroScan.utils.helpers import *\n",
    "from NeuroScan.constants.paths import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "db3edb3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class DataTransformationConfig:\n",
    "    root_dir: Path\n",
    "    source_data_dir: Path\n",
    "    batch_size: int\n",
    "    class_mode: str\n",
    "    saved_train_gen_path: str\n",
    "    saved_val_gen_path: str\n",
    "    saved_test_gen_path: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "54e94ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataConfigurationManager:\n",
    "    def __init__(self, config_file=CONFIG_PATH, params_file=PARAMS_PATH):\n",
    "        self.config = read_yaml(config_file)\n",
    "        self.params = read_yaml(params_file)\n",
    "        create_directories([self.config.artifacts_root])\n",
    "\n",
    "\n",
    "    def get_data_transformation_config(self) -> DataTransformationConfig:\n",
    "        config = self.config.transformation\n",
    "        params = self.params.transformation\n",
    "        \n",
    "        create_directories([config.root_dir])\n",
    "        \n",
    "        return DataTransformationConfig(\n",
    "            root_dir=Path(config.root_dir),\n",
    "            source_data_dir=Path(config.source_data_dir),\n",
    "            batch_size=params.batch_size,\n",
    "            class_mode=config.class_mode,\n",
    "            saved_train_gen_path=config.saved_train_gen_path,\n",
    "            saved_val_gen_path=config.saved_val_gen_path,\n",
    "            saved_test_gen_path=config.saved_test_gen_path\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d7e6eb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from dataclasses import dataclass\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from NeuroScan.utils.helpers import create_directories, read_yaml\n",
    "from NeuroScan.utils.logging import logger\n",
    "\n",
    "\n",
    "class DataTransformation:\n",
    "    def __init__(self, config: DataTransformationConfig):\n",
    "\n",
    "        self.config = config\n",
    "        self.classes = ['glioma', 'meningioma', 'notumor', 'pituitary']\n",
    "        self.cropped_train_dir = self.config.source_data_dir / 'Crop-Brain-MRI'\n",
    "        self.cropped_test_dir = self.config.source_data_dir / 'Test-Data'\n",
    "\n",
    "    def save_preprocessed_data(self, train_dir, val_dir, test_dir):\n",
    "\n",
    "        try:\n",
    "            train_datagen = ImageDataGenerator(\n",
    "                rotation_range=10,\n",
    "                height_shift_range=0.2,\n",
    "                horizontal_flip=True,\n",
    "                validation_split=0.2\n",
    "            )\n",
    "            test_datagen = ImageDataGenerator()\n",
    "            train_gen = train_datagen.flow_from_directory(\n",
    "                directory=str(train_dir),\n",
    "                target_size=(240, 240),\n",
    "                batch_size=self.config.batch_size,\n",
    "                class_mode=self.config.class_mode,\n",
    "                subset='training',\n",
    "                shuffle=False\n",
    "            )\n",
    "            val_gen = train_datagen.flow_from_directory(\n",
    "                directory=str(train_dir),\n",
    "                target_size=(240, 240),\n",
    "                batch_size=self.config.batch_size,\n",
    "                class_mode=self.config.class_mode,\n",
    "                subset='validation',\n",
    "                shuffle=False\n",
    "            )\n",
    "            test_gen = test_datagen.flow_from_directory(\n",
    "                directory=str(test_dir),\n",
    "                target_size=(240, 240),\n",
    "                batch_size=self.config.batch_size,\n",
    "                class_mode=self.config.class_mode,\n",
    "                shuffle=False\n",
    "            )\n",
    "\n",
    "            train_data = []\n",
    "            train_labels = []\n",
    "            val_data = []\n",
    "            val_labels = []\n",
    "            test_data = []\n",
    "            test_labels = []\n",
    "            for _ in range(train_gen.n // train_gen.batch_size + 1):\n",
    "                x, y = next(train_gen)\n",
    "                train_data.append(x)\n",
    "                train_labels.append(y)\n",
    "            for _ in range(val_gen.n // val_gen.batch_size + 1):\n",
    "                x, y = next(val_gen)\n",
    "                val_data.append(x)\n",
    "                val_labels.append(y)\n",
    "            for _ in range(test_gen.n // test_gen.batch_size + 1):\n",
    "                x, y = next(test_gen)\n",
    "                test_data.append(x)\n",
    "                test_labels.append(y)\n",
    "\n",
    "            train_data = np.concatenate(train_data, axis=0)\n",
    "            train_labels = np.concatenate(train_labels, axis=0)\n",
    "            val_data = np.concatenate(val_data, axis=0)\n",
    "            val_labels = np.concatenate(val_labels, axis=0)\n",
    "            test_data = np.concatenate(test_data, axis=0)\n",
    "            test_labels = np.concatenate(test_labels, axis=0)\n",
    "\n",
    "            np.save(self.config.saved_train_gen_path, {'data': train_data, 'labels': train_labels})\n",
    "            np.save(self.config.saved_val_gen_path, {'data': val_data, 'labels': val_labels})\n",
    "            np.save(self.config.saved_test_gen_path, {'data': test_data, 'labels': test_labels})\n",
    "            logger.info(f\"Preprocessed data saved to {self.config.saved_train_gen_path}, {self.config.saved_val_gen_path}, {self.config.saved_test_gen_path}\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error saving preprocessed data: {e}\")\n",
    "            raise\n",
    "\n",
    "    def load_preprocessed_data(self):\n",
    "        \"\"\"Loads preprocessed data if available.\"\"\"\n",
    "        try:\n",
    "            if (os.path.exists(self.config.saved_train_gen_path) and\n",
    "                os.path.exists(self.config.saved_val_gen_path) and\n",
    "                os.path.exists(self.config.saved_test_gen_path)):\n",
    "                train_data = np.load(self.config.saved_train_gen_path, allow_pickle=True).item()\n",
    "                val_data = np.load(self.config.saved_val_gen_path, allow_pickle=True).item()\n",
    "                test_data = np.load(self.config.saved_test_gen_path, allow_pickle=True).item()\n",
    "                logger.info(\"Preprocessed data loaded from saved files.\")\n",
    "                return train_data, val_data, test_data\n",
    "\n",
    "            logger.warning(\"No saved preprocessed data found; transformation required.\")\n",
    "            return None, None, None\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error loading preprocessed data: {e}\")\n",
    "            raise\n",
    "\n",
    "    def create_data_generators(self, train_data=None, val_data=None, test_data=None):\n",
    "        \"\"\"Creates data generators from preprocessed data or directories.\"\"\"\n",
    "\n",
    "        try:\n",
    "            if train_data is None or val_data is None or test_data is None:\n",
    "                logger.info(\"Creating generators from directories...\")\n",
    "                train_datagen = ImageDataGenerator(\n",
    "                    rotation_range=10,\n",
    "                    height_shift_range=0.2,\n",
    "                    horizontal_flip=True,\n",
    "                    validation_split=0.2\n",
    "                )\n",
    "                test_datagen = ImageDataGenerator()\n",
    "                train_generator = train_datagen.flow_from_directory(\n",
    "                    directory=str(self.cropped_train_dir),\n",
    "                    target_size=(240, 240),\n",
    "                    batch_size=self.config.batch_size,\n",
    "                    class_mode=self.config.class_mode,\n",
    "                    subset='training'\n",
    "                )\n",
    "                valid_generator = train_datagen.flow_from_directory(\n",
    "                    directory=str(self.cropped_train_dir),\n",
    "                    target_size=(240, 240),\n",
    "                    batch_size=self.config.batch_size,\n",
    "                    class_mode=self.config.class_mode,\n",
    "                    subset='validation'\n",
    "                )\n",
    "                test_generator = test_datagen.flow_from_directory(\n",
    "                    directory=str(self.cropped_test_dir),\n",
    "                    target_size=(240, 240),\n",
    "                    batch_size=self.config.batch_size,\n",
    "                    class_mode=self.config.class_mode,\n",
    "                    shuffle=False\n",
    "                )\n",
    "\n",
    "                # Validate cardinality for directory-based generators\n",
    "                if (train_generator.n == 0 or valid_generator.n == 0 or test_generator.n == 0):\n",
    "                    logger.error(\"One or more data generators are empty.\")\n",
    "                    raise ValueError(\"Data generators contain no images.\")\n",
    "                logger.info(f\"Data generators created: {train_generator.n} training, \"\n",
    "                            f\"{valid_generator.n} validation, {test_generator.n} test images.\")\n",
    "            else:\n",
    "                logger.info(\"Creating generators from preprocessed data...\")\n",
    "                train_generator = tf.data.Dataset.from_tensor_slices((train_data['data'], train_data['labels'])).batch(self.config.batch_size)\n",
    "                valid_generator = tf.data.Dataset.from_tensor_slices((val_data['data'], val_data['labels'])).batch(self.config.batch_size)\n",
    "                test_generator = tf.data.Dataset.from_tensor_slices((test_data['data'], test_data['labels'])).batch(self.config.batch_size)\n",
    "\n",
    "                if (train_generator.cardinality().numpy() == 0 or valid_generator.cardinality().numpy() == 0 or test_generator.cardinality().numpy() == 0):\n",
    "                    logger.error(\"One or more data generators are empty.\")\n",
    "                    raise ValueError(\"Data generators contain no images.\")\n",
    "                \n",
    "                logger.info(f\"Data generators created: {train_generator.cardinality().numpy() * self.config.batch_size} training, \"\n",
    "                            f\"{valid_generator.cardinality().numpy() * self.config.batch_size} validation, \"\n",
    "                            f\"{test_generator.cardinality().numpy() * self.config.batch_size} test images.\")\n",
    "\n",
    "            return train_generator, valid_generator, test_generator\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error creating data generators: {e}\")\n",
    "            raise\n",
    "\n",
    "    def transform(self):\n",
    "        try:\n",
    "            logger.info(\"Starting data transformation pipeline...\")\n",
    "            train_data, val_data, test_data = self.load_preprocessed_data()\n",
    "            if train_data is None:\n",
    "                self.save_preprocessed_data(self.cropped_train_dir, self.cropped_train_dir, self.cropped_test_dir)\n",
    "                train_data, val_data, test_data = self.load_preprocessed_data()\n",
    "            train_gen, valid_gen, test_gen = self.create_data_generators(train_data, val_data, test_data)\n",
    "            logger.info(\"Data transformation pipeline completed.\")\n",
    "            return train_gen, valid_gen, test_gen\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Data transformation failed: {e}\")\n",
    "            raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3480ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "        config = DataConfigurationManager()\n",
    "        transform_config = config.get_data_transformation_config()\n",
    "        transformer = DataTransformation(config=transform_config)\n",
    "        train_gen, valid_gen, test_gen = transformer.transform()\n",
    "except Exception as e:\n",
    "        logger.error(f\"Data transformation failed: {e}\")\n",
    "        raise"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NeuroScan",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
